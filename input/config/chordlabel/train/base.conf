# Config for bin/models/chordlabel/train.py
# Trains a chord labeling model

%% ABSTRACT

# Model name
%{model_name}
# Input data
%{PROJECT_ROOT}/input/midi/heads/gold_heads.csv

# Input type specification
filetype = bulk-midi-train
# Take chord corpus input from a file to init transition params
file-options = chords=%{PROJECT_ROOT}/input/fullseqs

# Set model training options
opts       = chordprob=0.8:\
             vocab=%{vocab}:\
             split=100:\
             trainprocs=4:\
             max_iterations=8:\
             convergence_logprob=0.01

log = %{PROJECT_ROOT}/etc/tmp/chordlabel/train/

# Note that this partitions the chord data used to initialize the 
#  models' transition distributions before EM
# The MIDI data doesn't need to be partitioned, since it's unlabeled
# When we test, we only use models that didn't have access to the test 
#  chord sequences for initialization
partitions = 10
